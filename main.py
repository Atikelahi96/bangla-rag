# main.py â€” hybrid RAG (Qdrant vectors + BM25 keyword)  â€¢  LangChain 0.2.2
# -----------------------------------------------------------------------
import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from dotenv import load_dotenv

from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain

# â”€â”€ 0. environment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# â”€â”€ 1. embeddings & vector store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

client = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY"),
)

vector_store = QdrantVectorStore(
    client=client,
    collection_name=os.getenv("QDRANT_COLLECTION"),
    embedding=embeddings,
    content_payload_key="text",
)

# â”€â”€ 2. hybrid retriever (vectors + BM25) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
vect_ret = vector_store.as_retriever(search_kwargs={"k": 12})  # â†‘k â†’ better recall

docs, offset = [], None
while True:                                   # pull every doc once for BM25
    hits, offset = client.scroll(
        os.getenv("QDRANT_COLLECTION"),
        offset=offset,
        with_payload=True,
        limit=512,
    )
    docs += [
        Document(
            page_content=h.payload["text"],
            metadata={
                "id": str(h.id),
                "preview": h.payload["text"][:40]
            },                               # â† no more empty {}
        )
        for h in hits
        if "text" in h.payload
    ]
    if offset is None:
        break

bm25_ret = BM25Retriever.from_documents(docs)
bm25_ret.k = 8

retriever = EnsembleRetriever(
    retrievers=[vect_ret, bm25_ret],
    weights=[0.6, 0.4],          # â†‘vector weight, â†“BM25 to damp OCR noise
)

# â”€â”€ 3. LLMs & chains â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm_answer   = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.4)
llm_condense = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0)

# 3a. answering chain (grounded prompt)
answer_prompt = PromptTemplate.from_template(
    "à¦ªà§à¦°à¦¸à¦™à§à¦— (à¦¶à§à¦§à§ à¦¨à¦¿à¦šà§‡à¦° à¦¤à¦¥à§à¦¯ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§à¦¨):\n{context}\n\n"
    "à¦ªà§à¦°à¦¶à§à¦¨: {question}\n\n"
    "à¦‰à¦¤à§à¦¤à¦° (à¦‰à¦¦à§à¦§à§ƒà¦¤à¦¿ à¦¸à¦¹ à¦²à¦¿à¦–à§à¦¨, à¦¯à¦¦à¦¿ à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦¤à¦¥à§à¦¯ à¦¨à¦¾ à¦¥à¦¾à¦•à§‡, à¦¤à¦¬à§‡ à¦…à¦¨à§à¦®à¦¾à¦¨à¦¯à§‹à¦—à§à¦¯ à¦¹à¦²à§‡ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨, à¦¨à¦‡à¦²à§‡ 'à¦‰à¦¤à§à¦¤à¦° à¦ªà¦¾à¦‡à¦¨à¦¿' à¦¬à¦²à§à¦¨à¥¤):"
)
answer_chain = StuffDocumentsChain(
    llm_chain=LLMChain(llm=llm_answer, prompt=answer_prompt),
    document_variable_name="context",
)

# 3b. follow-up question re-writer
condense_prompt = PromptTemplate.from_template(
    "à¦ªà§‚à¦°à§à¦¬à¦¬à¦°à§à¦¤à§€ à¦†à¦²à¦¾à¦ª:\n{chat_history}\n\nà¦¬à¦°à§à¦¤à¦®à¦¾à¦¨ à¦ªà§à¦°à¦¶à§à¦¨: {question}\n"
    "à¦¶à§à¦§à§ à¦¤à¦¥à§à¦¯à¦ªà§‚à¦°à§à¦£, à¦¸à§à¦¬à¦¯à¦¼à¦‚à¦¸à¦®à§à¦ªà§‚à¦°à§à¦£ à¦ªà§à¦°à¦¶à§à¦¨ à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§à¦¨ (à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼):"
)
question_generator = LLMChain(llm=llm_condense, prompt=condense_prompt)

# 3c. short-term memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    output_key="answer",
    return_messages=True,
)

# 3d. conversational RAG chain
rag_chain = ConversationalRetrievalChain(
    retriever=retriever,
    question_generator=question_generator,
    combine_docs_chain=answer_chain,
    memory=memory,
    output_key="answer",
    return_source_documents=True,
)

# â”€â”€ 4. FastAPI wrapper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AskRequest(BaseModel):
    question: str

app = FastAPI(title="Banglaâ€“English RAG (Hybrid)")

@app.post("/ask")
async def ask(payload: AskRequest):
    q = payload.question.strip()
    if not q:
        raise HTTPException(400, "Question cannot be empty.")
    result = rag_chain.invoke({"question": q})
    return {
        "answer":  result["answer"],
        "sources": [d.metadata for d in result["source_documents"]],
    }

@app.get("/")
def root():
    return {"status": "ok", "message": "Bangla RAG hybrid ready ğŸš€"}
